# **The Cognitive Canvas: Architecting a Next-Generation Multimodal Educational Environment**

## **1\. Executive Summary and Strategic Vision**

The digitization of education has historically followed a skeuomorphic trajectory, replicating physical artifacts—notebooks, whiteboards, and textbooks—within the confines of a glass screen. While functional, these tools have largely remained passive receptacles for information. The next frontier in educational technology requires a fundamental shift from passive capture to active cognition. We are witnessing the emergence of the "Cognitive Canvas," a digital workspace where the environment itself acts as a collaborative partner in the learning process. This report outlines the architectural blueprint for a Progressive Web Application (PWA) that leverages the convergence of infinite canvas technologies, pressure-sensitive digital ink, and the latest advancements in multimodal Artificial Intelligence, specifically Google’s Gemini 3 Pro, NanoBanana (Gemini 2.5 Flash Image), and LearnLM.

The proposed solution addresses a critical gap in the current ed-tech landscape: the disconnection between freeform ideation and structured verification. Students studying STEM disciplines typically work in two distinct modes. The first is the "messy" mode of scribbling free-body diagrams, deriving equations, and sketching circuits—tasks best suited for handwriting. The second is the "formal" mode of coding, typesetting, and verifying logic—tasks usually performed in rigid, linear text editors. The Cognitive Canvas bridges these modalities. It provides an unbounded, pressure-sensitive surface for handwriting (powered by the tldraw SDK) that is continuously monitored by an AI reasoning engine (Gemini 3 Pro).

This integration allows for workflows previously impossible in a single tool. A student can hand-draw a circuit, and the system will not only beautify the sketch into a standard schematic (using NanoBanana) but also analyze its viability, calculating resistance and identifying short circuits (using Gemini 3’s "Deep Think" capabilities). Simultaneously, the LearnLM model acts as a Socratic tutor, observing the student's work in real-time to offer pedagogical guidance rather than mere answer keys. The following analysis details the full-stack development strategy, encompassing frontend engineering, AI orchestration, and document intelligence pipelines required to bring this vision to reality.

## **2\. The Educational Landscape and User Needs Analysis**

### **2.1 The Limitations of Linear Digital Tools**

Current educational tools force a dichotomy between structure and freedom. Word processors and LaTeX editors offer structure but stifle the non-linear thinking required for complex problem solving. Conversely, digital whiteboards (like standard implementations of Excalidraw or Miro) offer freedom but lack the domain-specific intelligence to understand that a circle drawn around a variable in an equation is a semantic grouping, not just a geometric shape.

The target user—a university-level engineering or physics student—encounters friction when transitioning between these states. For instance, solving a differential equation involves multiple steps of algebraic manipulation that are cumbersome to type but trivial to write. However, verifying that derivation requires a computational engine. By embedding the computational engine _into_ the writing surface, we reduce the cognitive load associated with tool-switching. The "Infinite Canvas" paradigm is essential here because it breaks the artificial constraint of the "page." Ideas can branch, cluster, and evolve spatially, mirroring the cognitive map of the learner rather than the physical limitations of paper.1

### **2.2 The "Cognitive Canvas" User Journey**

To understand the technical requirements, we must dissect the user's interaction with the tool. Consider a typical tutorial session:

1. **Ingestion:** The student imports a PDF problem set. The system must render this not as a static background, but as an interactive layer within the infinite space.2
2. **Ideation (Handwriting):** The student uses a stylus to annotate the PDF. The system must capture this input with low latency, utilizing the PointerEvent API to register pressure and tilt, creating a natural writing experience indistinguishable from pen on paper.3
3. **Selection and Intent (The Wand Tool):** The student circles a specific problem or sketch using a "Wand" tool. This action triggers the AI pipeline. The system must interpret the _intent_ of the selection—is it a math problem to be solved, a diagram to be cleaned, or text to be summarized?.4
4. **Transformation and Feedback:**
   - **Math:** The messy handwriting is converted to a rendered LaTeX block for clarity, and the Gemini 3 model verifies the steps.
   - **Circuits:** The hand-drawn schematic is replaced by a crisp SVG diagram generated by NanoBanana or a structured diagramming tool.5
   - **Tutoring:** The LearnLM sidebar offers a hint: "Check the sign convention on the second loop of your circuit.".6
5. **Synthesis and Export:** The final workspace, containing a mix of original ink, AI-generated diagrams, and structured notes, is compiled into a paginated PDF for submission.

### **2.3 Technical Constraints and Requirements**

Building this application requires navigating several competing constraints. The tool must be a PWA to ensure accessibility across devices (tablets, desktops) without the friction of app store reviews, yet it must handle heavy graphics operations typically reserved for native apps. It must manage large vector datasets (ink strokes) and high-resolution bitmaps (PDF pages) in browser memory. Furthermore, it must secure API keys for enterprise-grade AI models while operating largely on the client side. The selection of the technology stack is therefore not merely a preference but a strategic necessity to meet these performance benchmarks.

## **3\. Core Architecture and Frontend Engineering**

### **3.1 The Progressive Web Application (PWA) Foundation**

The foundation of the Cognitive Canvas is a robust PWA architecture. We select **Next.js** (specifically version 15 or later) as the framework. Next.js offers a dual advantage: Server-Side Rendering (SSR) ensures rapid initial page loads and SEO-friendliness, while its API Routes (or Server Actions) provide a secure backend environment to proxy requests to Google Vertex AI, ensuring that sensitive API keys never leak to the client browser.7

To achieve a "native" feel on tablet devices—crucial for stylus interaction—the web app manifest must be meticulously configured. Setting "display": "standalone" removes the browser interface (URL bar, navigation buttons), dedicating 100% of the screen real estate to the canvas. Additionally, the CSS property touch-action: none must be applied globally to the canvas container. This is a critical implementation detail; without it, the browser's native gestures (like "swipe to go back" on iOS or "pull to refresh" on Android) will conflict with the user's drawing intent, ruining the handwriting experience.8

### **3.2 The Infinite Canvas Engine: tldraw SDK**

We eschew building a canvas engine from scratch in favor of the **tldraw SDK**. tldraw is distinct from other canvas libraries (like Fabric.js or Konva) because it provides a high-level abstraction for "whiteboard" primitives while remaining fully "headless".1 This means the core logic—state management, undo/redo stacks, keyboard shortcuts, and shape manipulation—is provided, but the User Interface (UI) is completely customizable.

The architecture of tldraw relies on a customized state machine. Every tool (Pen, Eraser, Selector) is a StateNode.9 This allows us to inject our custom logic seamlessly. For instance, the standard "Eraser" can be subclassed to perform "Smart Erase" functions (e.g., erasing entire semantic blocks rather than pixels), and new tools like the "Wand" can be registered as first-class citizens within the editor's state chart.

#### **3.2.1 Data Persistence and Synchronization**

Educational work is critical; data loss is unacceptable. We implement a "Local-First" data architecture. The tldraw store allows for listeners that can serialize the canvas state (a JSON snapshot of all shapes and assets) to the browser's IndexedDB on every change.10 This ensures that even if the network fails, the student's work is saved locally.

For the "multi-device" requirement (starting on a tablet, finishing on a desktop), we integrate **Yjs**, a Conflict-free Replicated Data Type (CRDT) library. Yjs binds directly to the tldraw store, handling the complex logic of merging updates from multiple sources. This enables real-time collaboration and synchronization via WebSockets (using a provider like Hocuspocus or Liveblocks) or Firestore, allowing a student to circle a problem on their iPad and see the AI feedback appear instantly on their laptop screen.10

### **3.3 Stylus Interaction and Pressure Sensitivity**

The difference between a toy app and a professional tool lies in the fidelity of the ink. The browser's **Pointer Events API** is the bridge between the hardware stylus and our software canvas.3 This API exposes critical data points: pressure (0.0 to 1.0), tiltX, tiltY, and azimuthAngle.

Standard mouse events are insufficient. We must listen for pointerdown, pointermove, and pointerup. However, raw pointer data is noisy. As the user draws, the digitizer samples points at a discrete rate (e.g., 60Hz or 120Hz), which can lead to jagged lines if rendered directly. The application must implement a stabilization pipeline. We utilize the perfect-freehand algorithm (integrated into tldraw) which performs spline interpolation (typically Catmull-Rom or similar) to generate a smooth polygon mesh from the input points. This mesh is dynamically tapered based on the pressure data—higher pressure creates a wider stroke, mimicking a fountain pen or marker.12

Palm Rejection Implementation:  
A critical UX challenge on tablets is palm rejection. While the OS handles some of this, the application must be robust. We implement a filter in the pointer event handler:

JavaScript

if (event.pointerType \=== 'touch' && app.getCurrentToolId() \=== 'pen') {  
 return; // Ignore touch inputs when the pen tool is active  
}

This simple logic enforces a "Pen Mode" where drawing is exclusive to the stylus, while touch inputs are reserved for panning and zooming the canvas. This significantly reduces accidental artifacts caused by the user resting their hand on the screen.13

### **3.4 Custom Shapes and Extensibility**

The default shapes (rectangle, arrow) are insufficient for a physics or math tutorial. We must extend the schema using tldraw's ShapeUtil class. This class defines how a shape is rendered, how it handles hit-testing, and how it behaves during resize operations.14

**Table 1: Proposed Custom Shapes for Cognitive Canvas**

| Shape Type      | Functionality                             | Rendering Technology      | Interaction Model                                                     |
| :-------------- | :---------------------------------------- | :------------------------ | :-------------------------------------------------------------------- |
| **LaTeXShape**  | Displays formatted math equations.        | KaTeX wrapped in React.   | Double-click to edit raw LaTeX; drag handle to resize font.           |
| **SmartPlot**   | Renders interactive function plots.       | FunctionPlot.js or D3.js. | Drag axis to change domain/range; AI updates formula.                 |
| **CircuitNode** | Displays schematic symbols (IEEE/IEC).    | Inline SVG.               | Snap points for connecting wires; properties panel for values (Ω, F). |
| **PDFPage**     | Renders a page from an uploaded document. | HTML Canvas (via pdf.js). | Locked by default; serves as a background layer for annotation.       |

Implementation of the LaTeXShape is particularly vital. We define a shape with a latex property. The component method of the ShapeUtil returns a React component that uses KaTeX to render this string. We choose KaTeX over MathJax for its superior rendering speed, which is essential when a student has dozens of equations on a single canvas.16 The shape's indicator method defines the bounding box for selection, ensuring that even complex integrals are easily manipulated as single objects.17

## **4\. The AI Reasoning Engine: Harnessing Gemini 3 Pro**

### **4.1 Model Capabilities and Selection**

The core intelligence of the platform drives the "Wand" tool. For this, **Gemini 3 Pro** is the unequivocal choice. Released in November 2025, this model represents a step-change in reasoning capabilities compared to its predecessors. It features a **1 million token context window**, allowing it to ingest entire textbooks or long distinct tutorial documents in a single prompt.18

Most critically, Gemini 3 Pro introduces a native **"Thinking Mode"** (configured via thinking_level="high"). Unlike standard LLMs that predict the next token immediately, Thinking Mode allows the model to engage in a silent, multi-step reasoning process before generating a response. This is non-negotiable for mathematical verification. When checking a student's derivation, the model must first solve the problem internally and then compare its correct path against the student's work step-by-step. Benchmarks validate this choice: Gemini 3 Pro scores **95.0%** on the AIME 2025 math benchmark (no tools) and **100%** with code execution enabled, outperforming GPT-5.1 in specific reasoning tasks.20

### **4.2 Multimodal Input Strategy: The "Make Real" Workflow**

The interaction flow for the Wand tool leverages the "Make Real" pattern, pioneered by tldraw prototypes.22 When the user circles an area:

1. **Capture:** The app calculates the bounding box of the selection.
2. **Rasterization:** It generates a high-fidelity PNG image of the selected area (including ink and background PDF content).24 While vector data (SVG) is cleaner, Gemini 3 Pro's vision encoder is optimized for pixel data, and rasterization preserves the visual context (like diagrams) better than raw SVG paths.4
3. **Context Injection:** The prompt includes not just the image but also textual context—metadata about the subject (e.g., "Physics 101") or the text of the PDF page extracted via pdf.js.
4. **Prompt Engineering:** We utilize a structured prompt strategy.
   - _Role:_ "You are an expert physics tutor and LaTeX typesetter."
   - _Instruction:_ "Analyze the handwritten notes in the image. Convert the mathematical expressions to LaTeX. Check the derivation for algebraic errors. If the circuit diagram is messy, generate a description for a clean version.".25

### **4.3 Structured Output and Controlled Generation**

A raw text response from the AI is useless for a programmable canvas. We need structured data to create shapes. Gemini 3 Pro supports **Structured Outputs** via JSON schema enforcement.26 We define a rigid schema for the API response:

JSON

{  
 "type": "object",  
 "properties": {  
 "classification": { "type": "string", "enum": \["math", "circuit", "text", "diagram"\] },  
 "latex_content": { "type": "string" },  
 "circuit_svg": { "type": "string" },  
 "feedback": { "type": "string" },  
 "confidence_score": { "type": "number" }  
 },  
 "required": \["classification", "confidence_score"\]  
}

By enforcing this schema, the application can deterministically parse the AI's output. If the classification is "math," the frontend automatically instantiates a LaTeXShape with the content from latex_content. If it's "circuit," it creates an SVGShape. This "JSON Mode" is critical for building reliable software on top of probabilistic models.28 Furthermore, utilizing **partial JSON streaming** allows the UI to provide feedback (e.g., "Analyzing equation...") as soon as the first tokens are received, reducing the perceived latency of the operation.30

## **5\. Visual Intelligence: Generative Editing with NanoBanana**

### **5.1 Generative Diagramming and Cleanup**

Students often struggle to draw neat diagrams. The user requirement includes "converting sketches to clean editable drawings" and "annotating images." For these visual tasks, we employ **Gemini 2.5 Flash Image**, internally codenamed **NanoBanana**.31 This model is optimized for speed and high-fidelity image manipulation, distinct from the reasoning-heavy Gemini 3 Pro.

### **5.2 The Masking and Inpainting Pipeline**

The "Wand" tool's visual editing capabilities rely on masking. When a user wants to modify a part of an image (e.g., "change this resistor to a capacitor" or "remove this shadow"), the workflow is as follows:

1. **User Action:** The user draws a "mask" over the target area using a specific brush or the selection tool.
2. **Mask Generation:** The frontend converts this vector path into a black-and-white bitmap (mask image) where white pixels represent the area to edit.33
3. **API Payload:** The request to Vertex AI includes three key components:
   - base_image: The original image (Base64 encoded).
   - mask_image: The generated mask (Base64 encoded).
   - prompt: The instruction (e.g., "Replace the hand-drawn resistor with a standard IEEE symbol").
4. **Processing:** NanoBanana uses the mask to constrain its diffusion process, regenerating only the selected pixels while maintaining seamless coherence with the surrounding image.34

### **5.3 Style Transfer and "Vibe Coding"**

The research highlights "Vibe Coding" and "AI Templates" as features of the new Gemini ecosystem.36 In our context, this enables **Style Transfer**. A student can draw a rough block diagram and prompt the system: "Make this look like a textbook illustration." The model uses the sketch as a **ControlNet**\-style structural guide (input image) and the text prompt to drive the aesthetic rendering (output image). This allows students to create professional-grade visual assets for their reports without needing advanced graphic design skills.32

## **6\. Pedagogical Alignment: The LearnLM Tutor**

### **6.1 From Chatbot to Tutor**

Standard LLMs function as oracles, providing direct answers. This is detrimental to education. We integrate **LearnLM**, a family of models fine-tuned on educational research principles, to transform the system into a Socratic tutor.6 LearnLM is specifically trained to "inspire active learning," "manage cognitive load," and "adapt to the learner".6

### **6.2 System Instructions and Persona Design**

The behavior of the AI is governed by rigorous system instructions. We configure the LearnLM endpoint with a specialized persona:

**System Instruction:** "You are a pedagogical assistant designed to foster productive struggle. Do not provide the final answer to a problem unless explicitly asked to 'reveal the solution' after an attempt. When a student asks for help, analyze their current work on the canvas. Identify the underlying concept (e.g., Conservation of Energy). Ask a guiding question that leads them to the next step of the derivation. If they make an algebraic error, indicate the line number of the error but do not correct it for them.".6

This instruction set ensures that the tool aids learning rather than enabling cheating. The API allows for different "settings" or "modes" (e.g., "Classroom" vs. "Self-Taught"), where the strictness of the hints can be adjusted based on the user's preference or the teacher's global settings.40

### **6.3 Context-Aware Sidebar Integration**

The interaction interface is a collapsible sidebar. Crucially, this chat is not isolated. When the student sends a message ("I'm stuck on this integral"), the application silently appends a context packet to the message. This packet includes:

1. **Visual Context:** A snapshot of the current viewport.
2. **Semantic Context:** The LaTeX strings of any math shapes currently visible.
3. **Document Context:** The text of the PDF page currently in view.

This allows LearnLM to "see" what the student is seeing, enabling it to respond: "I see you are integrating $x^2$, but have you considered the boundary conditions at $x=0$?" without the student needing to type out the problem.22

## **7\. Document Intelligence: The PDF Workflow**

### **7.1 The Challenge of Infinite Canvas PDF Support**

Integrating standard A4 documents into an infinite, zoomable canvas presents significant engineering challenges. Browsers do not render PDFs natively as DOM elements that can be easily layered with WebGL or SVG canvases. Attempts to overlay a transparent div for text selection often result in "drift," where the text highlight desynchronizes from the visual document during zooming or panning.41

### **7.2 Rendering Strategy: The "Page-as-Asset" Model**

The most robust solution, utilized by leading whiteboard applications, is to treat PDF pages as static image assets.2

1. **Parsing:** We use the **pdf.js** library to parse the uploaded file structure.
2. **Rasterization:** Each page is rendered to an off-screen HTML Canvas at a high scale factor (e.g., 2.0 or 3.0) to ensure text remains crisp even when the user zooms in.
3. **Asset Creation:** These renders are converted to Blob URLs and instantiated as ImageShape objects on the tldraw canvas.
4. **Layout:** The system automatically arranges these page shapes in a vertical column or grid, adding a locked property to prevent accidental movement during handwriting.

### **7.3 Annotation and Extraction**

Since the PDF pages are now images, the student can use the standard tldraw Pen tool to annotate directly "on top" of them. The challenge is extracting the text under the ink for the AI.  
When the "Wand" tool selects an area that overlaps a PDF page shape, the system performs a coordinate transform. It maps the canvas coordinates of the selection to the internal coordinates of the PDF image. It then crops that specific region of the source image. This cropped visual is sent to Gemini 3 Pro, which utilizes its OCR capabilities to read the printed text along with the handwritten annotation. This avoids the complexity of maintaining a separate text layer while leveraging the model's superior vision capabilities.4

### **7.4 Vector-Based Export Pipeline**

A key requirement is exporting the work as a high-quality, multi-page A4 PDF. A simple screenshot is insufficient; it would pixelate the vector ink. We must implement a vector synthesis pipeline.

1. **Frame Definition:** The user places "Frame" shapes (from the toolbar) to define the export boundaries.
2. **Vector Transcoding:** The export engine iterates through these frames. For each frame, it utilizes jspdf in conjunction with svg2pdf.
3. **Synthesis:** It places the background PDF image (compressed JPEG). Then, it overlays the SVG paths of the handwritten ink and the rendered SVG of any LaTeX or circuit shapes.
4. **Result:** The final output is a PDF where the text (if typed/LaTeX) is selectable and the handwriting is smooth vector data, suitable for printing or high-resolution grading.24

## **8\. Security, Privacy, and Deployment**

### **8.1 Secure API Architecture**

Directly exposing Vertex AI API keys in a client-side application is a severe security risk. The architecture requires a Proxy Server.  
Using Next.js API Routes (/pages/api/generate.ts), we build a secure middleware.

- **Authentication:** The proxy verifies the user's session token (e.g., via Firebase Auth or Clerk) before processing any request.
- **Rate Limiting:** To control costs and prevent abuse, we implement a token bucket algorithm (using Redis or Vercel KV) to limit the number of AI requests per user per minute.18
- **Secret Management:** API keys for Gemini and NanoBanana are stored as environment variables on the server (GEMINI_API_KEY), never sent to the browser.

### **8.2 Data Privacy and Enterprise Control**

Given the educational context, data privacy is paramount. We utilize the **Enterprise** endpoints of Vertex AI, which guarantee that the data sent for inference (student notes, uploaded textbooks) is _not_ used to train the foundation models.18 Additionally, we implement a "Transient Mode" for the AI proxy, where images sent for analysis are processed in memory and immediately discarded, ensuring no persistence of user data on the intermediate servers.

### **8.3 Deployment and Offline Capabilities**

The application is deployed to a global edge network (Vercel) to minimize latency. As a PWA, it utilizes a **Service Worker** to cache the application shell (HTML, CSS, JS) and the WASM binaries for tldraw and pdf.js. This allows the student to open the app and review their notes even without an internet connection. While the AI features ("Wand," "Chat") will require connectivity, the core "pen and paper" utility remains functional offline, syncing data to the cloud once the connection is restored.7

## **9\. Conclusion**

The Cognitive Canvas represents a sophisticated synthesis of modern web engineering and state-of-the-art artificial intelligence. By building upon the **tldraw SDK**, we secure a flexible, high-performance foundation for the "infinite" workspace. By integrating **Gemini 3 Pro**, we inject the necessary reasoning power to turn that workspace into a mathematical verifier. **NanoBanana** adds the creative visual flair, while **LearnLM** ensures the entire experience remains pedagogically sound.

The resulting application is not merely a tool for recording information, but a platform for _thinking_. It respects the unstructured nature of human ideation while providing the structured support of machine intelligence. For the engineering student sketching a circuit or the physics major deriving a formula, this tool removes the friction of the medium, allowing them to focus entirely on the mastery of the concept. This is the future of the digital notebook: infinite, intelligent, and inextricably woven into the fabric of learning.

#### **Works cited**

1. tldraw: Infinite Canvas SDK for React, accessed on November 20, 2025, [https://tldraw.dev/](https://tldraw.dev/)
2. PDF editor • tldraw Docs, accessed on November 20, 2025, [https://tldraw.dev/examples/pdf-editor](https://tldraw.dev/examples/pdf-editor)
3. is there a way to get pressure sensitivity of tablet thorugh javascript \- Stack Overflow, accessed on November 20, 2025, [https://stackoverflow.com/questions/20357520/is-there-a-way-to-get-pressure-sensitivity-of-tablet-thorugh-javascript](https://stackoverflow.com/questions/20357520/is-there-a-way-to-get-pressure-sensitivity-of-tablet-thorugh-javascript)
4. 7 examples of Gemini's multimodal capabilities in action \- Google Developers Blog, accessed on November 20, 2025, [https://developers.googleblog.com/en/7-examples-of-geminis-multimodal-capabilities-in-action/](https://developers.googleblog.com/en/7-examples-of-geminis-multimodal-capabilities-in-action/)
5. Gemini 3.0 Pro is pretty good at Circuit Diagrams : r/Bard \- Reddit, accessed on November 20, 2025, [https://www.reddit.com/r/Bard/comments/1p0g3ln/gemini_30_pro_is_pretty_good_at_circuit_diagrams/](https://www.reddit.com/r/Bard/comments/1p0g3ln/gemini_30_pro_is_pretty_good_at_circuit_diagrams/)
6. LearnLM | Gemini API \- Google AI for Developers, accessed on November 20, 2025, [https://ai.google.dev/gemini-api/docs/learnlm](https://ai.google.dev/gemini-api/docs/learnlm)
7. Exporting from TLDraw to PNG, JPG doesn't work \- Cloudron Forum, accessed on November 20, 2025, [https://forum.cloudron.io/topic/9410/exporting-from-tldraw-to-png-jpg-doesn-t-work](https://forum.cloudron.io/topic/9410/exporting-from-tldraw-to-png-jpg-doesn-t-work)
8. tldraw/tldraw: very good whiteboard SDK / infinite canvas SDK \- GitHub, accessed on November 20, 2025, [https://github.com/tldraw/tldraw](https://github.com/tldraw/tldraw)
9. Custom tool (sticker) \- tldraw SDK, accessed on November 20, 2025, [https://tldraw.dev/examples/custom-tool](https://tldraw.dev/examples/custom-tool)
10. Custom diagram interface with tldraw: Real-time and scalable \- Synergy Codes, accessed on November 20, 2025, [https://www.synergycodes.com/portfolio/custom-diagram-tool-with-tldraw](https://www.synergycodes.com/portfolio/custom-diagram-tool-with-tldraw)
11. Building with TLDraw Computer: AI, and Convex is More Fun Than a Cat Made of Confetti, accessed on November 20, 2025, [https://www.youtube.com/watch?v=ns-7JNtR9sI](https://www.youtube.com/watch?v=ns-7JNtR9sI)
12. Drawing and canvas interactions \- tldraw, accessed on November 20, 2025, [https://tldraw.dev/features/composable-primitives/drawing-and-canvas-interactions](https://tldraw.dev/features/composable-primitives/drawing-and-canvas-interactions)
13. \[feature\] Apple Pencil Support on iPad · Issue \#202 · tldraw/tldraw-v1 \- GitHub, accessed on November 20, 2025, [https://github.com/tldraw/tldraw-v1/issues/202](https://github.com/tldraw/tldraw-v1/issues/202)
14. tldraw • customization, accessed on November 20, 2025, [https://tldraw.dev/features/customization](https://tldraw.dev/features/customization)
15. Custom shape \- tldraw SDK, accessed on November 20, 2025, [https://tldraw.dev/examples/custom-shape](https://tldraw.dev/examples/custom-shape)
16. Rendering LaTeX in React Websites | by Adwait Purao \- Medium, accessed on November 20, 2025, [https://medium.com/@adwait.purao/rendering-latex-in-react-websites-7bba0fb4bb97](https://medium.com/@adwait.purao/rendering-latex-in-react-websites-7bba0fb4bb97)
17. @tldraw/curve \- npm, accessed on November 20, 2025, [https://www.npmjs.com/package/@tldraw/curve](https://www.npmjs.com/package/@tldraw/curve)
18. Gemini 3 Pro | Generative AI on Vertex AI \- Google Cloud Documentation, accessed on November 20, 2025, [https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/3-pro](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/3-pro)
19. Gemini 3 Pro Preview – Vertex AI \- Google Cloud Console, accessed on November 20, 2025, [https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/gemini-3-pro-preview](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/gemini-3-pro-preview)
20. Google Gemini 3 Benchmarks \- Vellum AI, accessed on November 20, 2025, [https://www.vellum.ai/blog/google-gemini-3-benchmarks](https://www.vellum.ai/blog/google-gemini-3-benchmarks)
21. Gemini 3 Pro \- Google DeepMind, accessed on November 20, 2025, [https://deepmind.google/models/gemini/pro/](https://deepmind.google/models/gemini/pro/)
22. make real • tldraw, accessed on November 20, 2025, [https://makereal.tldraw.com/](https://makereal.tldraw.com/)
23. tldraw.computer \- Steve Ruiz, tldraw \- YouTube, accessed on November 20, 2025, [https://www.youtube.com/watch?v=1C2TdPkj6aQ](https://www.youtube.com/watch?v=1C2TdPkj6aQ)
24. Export canvas as image • tldraw Docs, accessed on November 20, 2025, [https://tldraw.dev/examples/export-canvas-as-image](https://tldraw.dev/examples/export-canvas-as-image)
25. Design multimodal prompts | Generative AI on Vertex AI \- Google Cloud Documentation, accessed on November 20, 2025, [https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/design-multimodal-prompts](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/design-multimodal-prompts)
26. Generate structured output (like JSON and enums) using the Gemini API | Firebase AI Logic, accessed on November 20, 2025, [https://firebase.google.com/docs/ai-logic/generate-structured-output](https://firebase.google.com/docs/ai-logic/generate-structured-output)
27. Structured output | Generative AI on Vertex AI \- Google Cloud Documentation, accessed on November 20, 2025, [https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/control-generated-output](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/control-generated-output)
28. How to consistently output JSON with the Gemini API using controlled generation \- Medium, accessed on November 20, 2025, [https://medium.com/google-cloud/how-to-consistently-output-json-with-the-gemini-api-using-controlled-generation-887220525ae0](https://medium.com/google-cloud/how-to-consistently-output-json-with-the-gemini-api-using-controlled-generation-887220525ae0)
29. JSON prompting for LLMs \- IBM Developer, accessed on November 20, 2025, [https://developer.ibm.com/articles/json-prompting-llms](https://developer.ibm.com/articles/json-prompting-llms)
30. Structured Outputs | Gemini API \- Google AI for Developers, accessed on November 20, 2025, [https://ai.google.dev/gemini-api/docs/structured-output](https://ai.google.dev/gemini-api/docs/structured-output)
31. Nano Banana – High-Quality AI Image Editing Model \- VisualGPT, accessed on November 20, 2025, [https://visualgpt.io/ai-models/nano-banana](https://visualgpt.io/ai-models/nano-banana)
32. Use Gemini 2.5 Flash Image (nano banana) on Vertex AI | Google Cloud Blog, accessed on November 20, 2025, [https://cloud.google.com/blog/products/ai-machine-learning/gemini-2-5-flash-image-on-vertex-ai](https://cloud.google.com/blog/products/ai-machine-learning/gemini-2-5-flash-image-on-vertex-ai)
33. Imagen for Editing and Customization – Vertex AI \- Google Cloud Console, accessed on November 20, 2025, [https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/imagen-3.0-capability-002](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/imagen-3.0-capability-002)
34. Customize images | Generative AI on Vertex AI \- Google Cloud Documentation, accessed on November 20, 2025, [https://docs.cloud.google.com/vertex-ai/generative-ai/docs/model-reference/imagen-api-customization](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/model-reference/imagen-api-customization)
35. Edit image content using a mask with Imagen v.002 | Generative AI on Vertex AI | Google Cloud Documentation, accessed on November 20, 2025, [https://docs.cloud.google.com/vertex-ai/generative-ai/docs/samples/generativeaionvertexai-imagen-edit-image-mask](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/samples/generativeaionvertexai-imagen-edit-image-mask)
36. Gemini 3 is available for enterprise | Google Cloud Blog, accessed on November 20, 2025, [https://cloud.google.com/blog/products/ai-machine-learning/gemini-3-is-available-for-enterprise](https://cloud.google.com/blog/products/ai-machine-learning/gemini-3-is-available-for-enterprise)
37. Google Photos gets Nano Banana and AI templates: Here's how you can edit and search images, accessed on November 20, 2025, [https://timesofindia.indiatimes.com/technology/tech-news/google-photos-gets-nano-banana-and-ai-templates-heres-how-you-can-edit-and-search-images/articleshow/125293232.cms](https://timesofindia.indiatimes.com/technology/tech-news/google-photos-gets-nano-banana-and-ai-templates-heres-how-you-can-edit-and-search-images/articleshow/125293232.cms)
38. Imagen 3 | Generative AI on Vertex AI \- Google Cloud Documentation, accessed on November 20, 2025, [https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/imagen/3-0-generate](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/imagen/3-0-generate)
39. System instructions | Generative AI on Vertex AI \- Google Cloud Documentation, accessed on November 20, 2025, [https://docs.cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/system-instruction-introduction](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/system-instruction-introduction)
40. LearnLM: Improving Gemini for Learning \- arXiv, accessed on November 20, 2025, [https://arxiv.org/html/2412.16429v3](https://arxiv.org/html/2412.16429v3)
41. Using PDF.js Layers in React.js Effortlessly, accessed on November 20, 2025, [https://blog.react-pdf.dev/understanding-pdfjs-layers-and-how-to-use-them-in-reactjs](https://blog.react-pdf.dev/understanding-pdfjs-layers-and-how-to-use-them-in-reactjs)
42. Customize the buttons to switch the scroll mode \- React PDF Viewer, accessed on November 20, 2025, [https://react-pdf-viewer.dev/examples/customize-the-buttons-to-switch-the-scroll-mode/](https://react-pdf-viewer.dev/examples/customize-the-buttons-to-switch-the-scroll-mode/)
43. Custom shape SVG export \- tldraw SDK, accessed on November 20, 2025, [https://tldraw.dev/examples/toSvg-method-example](https://tldraw.dev/examples/toSvg-method-example)
